{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<src img='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/dod-qa-sum-logo2.png'>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to `True` for those processes to initiate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_index_rebuild = True\n",
    "process_pdf_ocr = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -q pdf2image pytesseract reportlab pinecone-client Pillow gradio langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define some key variable for the PDF OCR'ing process\n",
    "#Configure paths and variables\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\" #tesseract must be installed separately\n",
    "POPPLER_PATH = r\"C:\\Program Files\\poppler-23.01.0\\Library\\bin\" #poppler must be installed separately\n",
    "\n",
    "LLM_DIRECTORY = Path(r\"C:\\Temp\\pdf_to_openai_chat\")\n",
    "LLM_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store the PDFs you want processed by this solution in this folder, as a *.pdf file only:\n",
    "SRC_PDFS_DIRECTORY = LLM_DIRECTORY / \"src_pdfs\"\n",
    "\n",
    "PAGES_DIRECTORY = LLM_DIRECTORY / \"pages\"\n",
    "PAGES_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_OUTPUT_DIRECTORY = LLM_DIRECTORY / \"text_output\"\n",
    "TEXT_OUTPUT_DIRECTORY.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF image OCR process, saving the results to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import pytesseract\n",
    "from PIL import Image, ImageOps\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "def create_page_images_pdf2image(pdf_path, output_directory, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF file into a list of JPEG images, one for each page.\n",
    "    \n",
    "    :param pdf_path: Path to the input PDF file.\n",
    "    :param output_directory: Directory to save the generated JPEG images.\n",
    "    :param dpi: DPI for the generated images.\n",
    "    :return: A list of file paths for the generated images.\n",
    "    \"\"\"\n",
    "    image_file_list = []\n",
    "    convert_args = {\"pdf_path\": pdf_path, \"dpi\": dpi}\n",
    "\n",
    "    if platform.system() == \"Windows\":\n",
    "        convert_args[\"poppler_path\"] = POPPLER_PATH\n",
    "\n",
    "    pdf_pages = convert_from_path(**convert_args)\n",
    "\n",
    "    for page_number, page in enumerate(pdf_pages, start=1):\n",
    "        output_file = output_directory / f\"page_{page_number:03}.jpg\"\n",
    "        page.save(output_file, \"JPEG\")\n",
    "        image_file_list.append(output_file)\n",
    "        #print(f\"Image created: {output_file}\")\n",
    "\n",
    "    return image_file_list\n",
    "\n",
    "\n",
    "def convert_to_bw(image_list):\n",
    "    \"\"\"\n",
    "    Convert a list of images to black and white.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    \"\"\"\n",
    "    for image_file in image_list:\n",
    "        image = Image.open(image_file)\n",
    "        gray_image = ImageOps.grayscale(image)\n",
    "        bw_image = gray_image.point(lambda x: 0 if x < 128 else 255, '1')\n",
    "        bw_image.save(image_file)\n",
    "\n",
    "\n",
    "def ocr_images(image_list, text_output_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on a list of images and append the extracted text to a file.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    :param text_output_path: File path to save the extracted text.\n",
    "    \"\"\"\n",
    "    with open(text_output_path, \"a\") as output_file:\n",
    "        for image_file in image_list:\n",
    "            text = str(((pytesseract.image_to_string(Image.open(image_file)))))\n",
    "            text = text.replace(\"-\\n\", \"\")\n",
    "            output_file.write(text)\n",
    "            #print(f\"OCR processed: {image_file}\")\n",
    "\n",
    "\n",
    "if process_pdf_ocr:\n",
    "    pdf_files = SRC_PDFS_DIRECTORY.glob(\"*.pdf\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_name = pdf_file.stem\n",
    "        \n",
    "        pages_subdir = PAGES_DIRECTORY / pdf_name\n",
    "        pages_subdir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image_file_list = create_page_images_pdf2image(pdf_file, pages_subdir, dpi=300)\n",
    "        print(f'Completed PDF2Image Page Creation for {pdf_file}')\n",
    "        \n",
    "        convert_to_bw(image_file_list)\n",
    "        print(f'Completed BWConvert for {pdf_file}')\n",
    "        \n",
    "        text_output_file = TEXT_OUTPUT_DIRECTORY / f\"{pdf_name}_text.txt\"\n",
    "          \n",
    "        ocr_images(image_file_list, text_output_file)\n",
    "        print(f'Completed OCRing file {pdf_file}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean some line-returns out of the text for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "TEXT_OUTPUT_DIRECTORY = r\"C:\\Temp\\pdf_to_openai_chat\\text_output\"\n",
    "\n",
    "def clean_text_file(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    cleaned_text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "if process_pdf_ocr:\n",
    "    for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "            output_file = os.path.join(TEXT_OUTPUT_DIRECTORY, f\"{os.path.splitext(filename)[0]}_cleaned.txt\")\n",
    "            clean_text_file(input_file, output_file)\n",
    "            os.remove(input_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our data (the process of OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# loop through all the files in the directory\n",
    "for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "        print(f\"Processing file: {filepath}\")\n",
    "        \n",
    "        # load the text file using the TextLoader\n",
    "        loader = TextLoader(filepath)\n",
    "        data = loader.load()\n",
    "        \n",
    "        print (f'You have {len(data)} document(s) in your data')\n",
    "        print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "        \n",
    "        # # print each metadata key and their values\n",
    "        # for key, value in data[0].metadata.items():\n",
    "        #     print(f'metadata: {key} = {value}')\n",
    "\n",
    "\n",
    "\n",
    "def data_doc_summerizer(docs):\n",
    "    print (f'You have {len(docs)} document(s)')\n",
    "    \n",
    "    num_words = sum([len(doc.page_content.split(' ')) for doc in docs])\n",
    "    \n",
    "    print (f'You have roughly {num_words} words in your docs')\n",
    "    print ()\n",
    "    print (f'Preview: \\n{docs[0].page_content.split(\". \")[0]}')\n",
    "\n",
    "data_doc_summerizer(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "chunking_size = 1536\n",
    "namespace_chunk_name = f'chunk_size_{chunking_size}'\n",
    "\n",
    "data = []  # list to store the loaded data for all text files\n",
    "text_chunks = []  # list to store the split text chunks for all text files\n",
    "\n",
    "# loop through all the files in the directory\n",
    "for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "        print(f\"Processing file: {filepath}\")\n",
    "        \n",
    "        # load the text file using the TextLoader\n",
    "        loader = TextLoader(filepath)\n",
    "        loaded_data = loader.load()\n",
    "        \n",
    "        # add the loaded data to the list\n",
    "        data.extend(loaded_data)\n",
    "        \n",
    "        # split the page content of the loaded data into smaller chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunking_size, chunk_overlap=20)\n",
    "        text_chunks.extend(text_splitter.split_documents(loaded_data))\n",
    "        \n",
    "# print some information about the loaded data and text chunks\n",
    "print(f'Loaded {len(data)} documents from {len(os.listdir(TEXT_OUTPUT_DIRECTORY))} text files')\n",
    "print(f'Got {len(text_chunks)} text chunks in total')\n",
    "\n",
    "# # example loop to print metadata for each document in the data list\n",
    "# for document in data:\n",
    "#     print(f\"Metadata for document in {document.metadata['source']}:\\n{document.metadata}\\n\")\n",
    "\n",
    "# # example loop to print the length of each text chunk\n",
    "# for i, chunk in enumerate(text_chunks):\n",
    "#     print(f\"Length of text chunk {i}: {len(chunk.page_content)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# We need to get the OpenAI or Azure OpenAI API key. This is how we use & get charged for LLM usage\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "else:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone is a service that will take the documents that you split\n",
    "# And store the embedding vectors (a math construct that tells LLMs where in the model to find similar words)\n",
    "# Currently, Pinecone is free for use cases like this. Other vector stores exist; FAISS, ChromaDB, etc.\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "PINECONE_API_ENV = \"us-central1-gcp\"\n",
    "\n",
    "# We need to get the Pinecone API key.\n",
    "if \"PINECONE_API_KEY\" in os.environ:\n",
    "    PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "else:\n",
    "    PINECONE_API_KEY = getpass(\"Enter your Pinecone API Key: \")\n",
    "    os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "index_name = 'aaa-reports-002'\n",
    "\n",
    "# Remove our Pinecone Index if it exists and create a new one.\n",
    "if process_index_rebuild:\n",
    "    try:\n",
    "        pinecone.delete_index(name=index_name)\n",
    "    except:\n",
    "        print(f\"The index {index_name} does not exist.\")\n",
    "    # Create an index (a database) for our embeddings\n",
    "    pinecone.create_index(name=index_name, dimension=1536, metric=\"cosine\")\n",
    "    print(f\"The index {index_name} was created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "# Let's create the embeddings (vector math pointers of our docs) using OpenAI's Embeddings Creator model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "# create an empty list to store the metadata for each document\n",
    "metadata_list = []\n",
    "\n",
    "# loop through the text_chunks and set the metadata for each document\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    # get the filename of the source text file from the metadata of the first page in the chunk\n",
    "    filename = chunk.metadata['source'].split(os.sep)[-1].split('.')[0]\n",
    "    # set the sources metadata key to the filename\n",
    "    metadata = {\"source\": filename}\n",
    "    # add the metadata dictionary to the metadata_list\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "# Create the docsearch which both builds the index and provides us an object to use the index.\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, metadatas=metadata_list, index_name=index_name, namespace=namespace_chunk_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the pre-trained question-answering LLM model using the langchain library\n",
    "llm = OpenAI(temperature=0.1, openai_api_key=OPENAI_API_KEY, max_tokens=1024)\n",
    "\n",
    "# Create a list of unique document titles from the metadata of the text chunks\n",
    "doc_titles = list(set(metadata['source'] for metadata in metadata_list))\n",
    "\n",
    "# Define custom prompt template: QA-STUFF Prompt\n",
    "qa_stuff_prompt_template = \"\"\"You are a military specialist. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer as a military expert in after-action reports.:\n",
    "\"\"\"\n",
    "QA_STUFF_PROMPT = PromptTemplate(\n",
    "    template=qa_stuff_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define custom prompt template: SUMMARIZE-MAP_REDUCE Prompt\n",
    "summarize_mr_prompt_template = \"\"\"Write a summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "SUMMARIZE_MR_PROMPT = PromptTemplate(template=summarize_mr_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create our CHAIN object for QA_STUFFed Prompts\n",
    "chain_qa = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=QA_STUFF_PROMPT)\n",
    "\n",
    "# Create our CHAIN object for SUMMARIZED_MR Prompts\n",
    "chain_summarize = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", return_intermediate_steps=False, map_prompt=SUMMARIZE_MR_PROMPT, combine_prompt=SUMMARIZE_MR_PROMPT)\n",
    "\n",
    "# Create our Function to process QA (mode) prompts\n",
    "def qa_function(query, doc_title, k):\n",
    "\n",
    "    # Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "    \n",
    "    metadata_filter = {\"source\": doc_title}\n",
    "    \n",
    "    qa_docs_to_search_with_scores = docsearch.similarity_search_with_score(\n",
    "        query=query, k=k, filter=metadata_filter, namespace=\"chunk_size_512\"\n",
    "    )\n",
    "\n",
    "    # Separate the documents from their scores\n",
    "    qa_docs_to_search = [doc for doc, score in qa_docs_to_search_with_scores]\n",
    "\n",
    "    # Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "    output = chain_qa({\"input_documents\": qa_docs_to_search, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "    # Extract the actual answer text from the output dictionary\n",
    "    answer = output['output_text']\n",
    "\n",
    "    # Extract the docs that the similarity_search found, scored\n",
    "    truncated_docs_str = \"\"\n",
    "    for index, (doc, score) in enumerate(qa_docs_to_search_with_scores):\n",
    "        truncated_text = doc.page_content[:200].replace(\"\\n\", \"\")\n",
    "        truncated_docs_str += f\"Doc {index + 1} snippet (score: {score:.2f}): {truncated_text}\\n------\\n\"\n",
    "\n",
    "    return answer, truncated_docs_str\n",
    "\n",
    "\n",
    "# Create our Function to process Summarization (mode) prompts\n",
    "def summarization_function(query, doc_title, k):\n",
    "\n",
    "    # Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "    \n",
    "    metadata_filter = {\"source\": doc_title}\n",
    "    \n",
    "    sum_docs_to_search_with_scores = docsearch.similarity_search_with_score(\n",
    "        query=query, k=k, filter=metadata_filter, namespace=\"chunk_size_512\"\n",
    "    )\n",
    "\n",
    "    # Separate the documents from their scores\n",
    "    sum_docs_to_search = [doc for doc, score in sum_docs_to_search_with_scores]\n",
    "\n",
    "    # Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "    output = chain_summarize({\"input_documents\": sum_docs_to_search}, return_only_outputs=False)\n",
    "\n",
    "    # Extract the actual answer text from the output dictionary\n",
    "    answer = output['output_text']\n",
    "\n",
    "    # Extract the docs that the similarity_search found, scored\n",
    "    truncated_docs_str = \"\"\n",
    "    for index, (doc, score) in enumerate(sum_docs_to_search_with_scores):\n",
    "        truncated_text = doc.page_content[:200].replace(\"\\n\", \"\")\n",
    "        truncated_docs_str += f\"Doc {index + 1} snippet (score: {score:.2f}): {truncated_text}\\n------\\n\"\n",
    "\n",
    "    return answer, truncated_docs_str\n",
    "\n",
    "\n",
    "# Function to determine which Function to use: qa or summarization along with the vars needed (order must match the list order provided to gradio)\n",
    "def process_input(mode, query, doc_title, k):\n",
    "    if mode == \"Question/Answer\":\n",
    "        return qa_function(query, doc_title, k)\n",
    "    elif mode == \"Summarization\":\n",
    "        return summarization_function(query, doc_title, k)\n",
    "    else:\n",
    "        return \"Invalid mode selected\", \"\"\n",
    "\n",
    "\n",
    "# The main function to launch the gradio interface\n",
    "def main():\n",
    "    with gr.Interface(\n",
    "        fn=process_input,\n",
    "        inputs=[\n",
    "            gr.Radio(choices=[\"Question/Answer\", \"Summarization\"], label=\"Model Interaction Mode\", default=\"Summarization\"),\n",
    "            gr.Textbox(lines=1, label=\"Question\"),\n",
    "            gr.Dropdown(choices=doc_titles, label=\"Filter by AAR title\", info=\"Filter by after-action report name\"),\n",
    "            gr.Slider(3, 30, step=1, value=5, label=\"Docs to Search\", info=\"Number of AAR document 'chunks' to search and summarize\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Answer\", lines=10),\n",
    "            gr.Textbox(label=\"Docs presented to the LLM\", info=\"A list of document 'chunks' from the AAR with its question-similarity score.\"),\n",
    "        ],\n",
    "        title=\"After-Action Report ChatGPT\",\n",
    "        examples=[\n",
    "            [\"Question/Answer\", \"Echeloning and it's use in Desert Shield and Storm.\", \"aar-desertStorm_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"What were the DDS problems?\", \"aar-desertStorm_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"Tell me about Brown and Root Corporation.\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"Can you classify the US military equipment, like tanks and helicopters used in Somalia by their type? Respond in a table.\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"List the battalions involved with Somalia. Confirm your list refers to US military battalions only.\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Summarization\", \"Brown and Root\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Summarization\", \"Issues related to SEABEE uniforms.\", \"aar-desertStorm_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"Tell me about Osman Atto\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"Based on what you know about Osman Atto, write a briefing to US Command officials.\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Summarization\", \"Response to Hurrican Sandy\", \"aar-hurricanSandy_text_cleaned\"],\n",
    "            [\"Summarization\", \"UNOSOM II\", \"aar-Somalia_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"List the key US military equipment deployed in Somalia, as a bulleted list.\",\"aar-Somalia_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"How long did the air war last?\", \"aar-desertStorm_text_cleaned\"]\n",
    "        ],\n",
    "    ) as iface:\n",
    "        iface.launch()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
