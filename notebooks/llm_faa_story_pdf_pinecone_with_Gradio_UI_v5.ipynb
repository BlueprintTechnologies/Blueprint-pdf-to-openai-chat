{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<src img='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai_2.png'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change to `True` if you want to process all the PDFs/OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_index_rebuild = True\n",
    "process_pdf_ocr = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pdf2image pytesseract reportlab pinecone-client Pillow gradio langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define some key variable for the PDF OCR'ing process\n",
    "#Configure paths and variables\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\" #tesseract must be installed separately\n",
    "POPPLER_PATH = r\"C:\\Program Files\\poppler-23.01.0\\Library\\bin\" #poppler must be installed separately\n",
    "\n",
    "LLM_DIRECTORY = Path(r\"C:\\Temp\\pdf_to_openai_chat\")\n",
    "LLM_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Store the PDFs you want processed by this solution in this folder, as a *.pdf file only:\n",
    "SRC_PDFS_DIRECTORY = LLM_DIRECTORY / \"src_pdfs\"\n",
    "\n",
    "PAGES_DIRECTORY = LLM_DIRECTORY / \"pages\"\n",
    "PAGES_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_OUTPUT_DIRECTORY = LLM_DIRECTORY / \"text_output\"\n",
    "TEXT_OUTPUT_DIRECTORY.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF image OCR process, saving the results to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import pytesseract\n",
    "from PIL import Image, ImageOps\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "def create_page_images_pdf2image(pdf_path, output_directory, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF file into a list of JPEG images, one for each page.\n",
    "    \n",
    "    :param pdf_path: Path to the input PDF file.\n",
    "    :param output_directory: Directory to save the generated JPEG images.\n",
    "    :param dpi: DPI for the generated images.\n",
    "    :return: A list of file paths for the generated images.\n",
    "    \"\"\"\n",
    "    image_file_list = []\n",
    "    convert_args = {\"pdf_path\": pdf_path, \"dpi\": dpi}\n",
    "\n",
    "    if platform.system() == \"Windows\":\n",
    "        convert_args[\"poppler_path\"] = POPPLER_PATH\n",
    "\n",
    "    pdf_pages = convert_from_path(**convert_args)\n",
    "\n",
    "    for page_number, page in enumerate(pdf_pages, start=1):\n",
    "        output_file = output_directory / f\"page_{page_number:03}.jpg\"\n",
    "        page.save(output_file, \"JPEG\")\n",
    "        image_file_list.append(output_file)\n",
    "        #print(f\"Image created: {output_file}\")\n",
    "\n",
    "    return image_file_list\n",
    "\n",
    "\n",
    "def convert_to_bw(image_list):\n",
    "    \"\"\"\n",
    "    Convert a list of images to black and white.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    \"\"\"\n",
    "    for image_file in image_list:\n",
    "        image = Image.open(image_file)\n",
    "        gray_image = ImageOps.grayscale(image)\n",
    "        bw_image = gray_image.point(lambda x: 0 if x < 128 else 255, '1')\n",
    "        bw_image.save(image_file)\n",
    "\n",
    "\n",
    "def ocr_images(image_list, text_output_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on a list of images and append the extracted text to a file.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    :param text_output_path: File path to save the extracted text.\n",
    "    \"\"\"\n",
    "    with open(text_output_path, \"a\") as output_file:\n",
    "        for image_file in image_list:\n",
    "            text = str(((pytesseract.image_to_string(Image.open(image_file)))))\n",
    "            text = text.replace(\"-\\n\", \"\")\n",
    "            output_file.write(text)\n",
    "            #print(f\"OCR processed: {image_file}\")\n",
    "\n",
    "\n",
    "if process_pdf_ocr:\n",
    "    pdf_files = SRC_PDFS_DIRECTORY.glob(\"*.pdf\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_name = pdf_file.stem\n",
    "        \n",
    "        pages_subdir = PAGES_DIRECTORY / pdf_name\n",
    "        pages_subdir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        image_file_list = create_page_images_pdf2image(pdf_file, pages_subdir, dpi=300)\n",
    "        print(f'Completed PDF2Image Page Creation for {pdf_file}')\n",
    "        \n",
    "        convert_to_bw(image_file_list)\n",
    "        print(f'Completed BWConvert for {pdf_file}')\n",
    "        \n",
    "        text_output_file = TEXT_OUTPUT_DIRECTORY / f\"{pdf_name}_text.txt\"\n",
    "          \n",
    "        ocr_images(image_file_list, text_output_file)\n",
    "        print(f'Completed OCRing file {pdf_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "TEXT_OUTPUT_DIRECTORY = r\"C:\\Temp\\pdf_to_openai_chat\\text_output\"\n",
    "\n",
    "def clean_text_file(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    cleaned_text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "if process_pdf_ocr:\n",
    "    for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "            output_file = os.path.join(TEXT_OUTPUT_DIRECTORY, f\"{os.path.splitext(filename)[0]}_cleaned.txt\")\n",
    "            clean_text_file(input_file, output_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our data (the process of OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# loop through all the files in the directory\n",
    "for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "        print(f\"Processing file: {filepath}\")\n",
    "        \n",
    "        # load the text file using the TextLoader\n",
    "        loader = TextLoader(filepath)\n",
    "        data = loader.load()\n",
    "        \n",
    "        print (f'You have {len(data)} document(s) in your data')\n",
    "        print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "        \n",
    "        # # print each metadata key and their values\n",
    "        # for key, value in data[0].metadata.items():\n",
    "        #     print(f'metadata: {key} = {value}')\n",
    "\n",
    "\n",
    "\n",
    "def data_doc_summerizer(docs):\n",
    "    print (f'You have {len(docs)} document(s)')\n",
    "    \n",
    "    num_words = sum([len(doc.page_content.split(' ')) for doc in docs])\n",
    "    \n",
    "    print (f'You have roughly {num_words} words in your docs')\n",
    "    print ()\n",
    "    print (f'Preview: \\n{docs[0].page_content.split(\". \")[0]}')\n",
    "\n",
    "data_doc_summerizer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "chunking_size = 1024\n",
    "namespace_chunk_name = f'chunk_size_{chunking_size}'\n",
    "\n",
    "data = []  # list to store the loaded data for all text files\n",
    "text_chunks = []  # list to store the split text chunks for all text files\n",
    "\n",
    "# loop through all the files in the directory\n",
    "for filename in os.listdir(TEXT_OUTPUT_DIRECTORY):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(TEXT_OUTPUT_DIRECTORY, filename)\n",
    "        print(f\"Processing file: {filepath}\")\n",
    "        \n",
    "        # load the text file using the TextLoader\n",
    "        loader = TextLoader(filepath)\n",
    "        loaded_data = loader.load()\n",
    "        \n",
    "        # add the loaded data to the list\n",
    "        data.extend(loaded_data)\n",
    "        \n",
    "        # split the page content of the loaded data into smaller chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunking_size, chunk_overlap=20)\n",
    "        text_chunks.extend(text_splitter.split_documents(loaded_data))\n",
    "        \n",
    "# print some information about the loaded data and text chunks\n",
    "print(f'Loaded {len(data)} documents from {len(os.listdir(TEXT_OUTPUT_DIRECTORY))} text files')\n",
    "print(f'Got {len(text_chunks)} text chunks in total')\n",
    "\n",
    "# # example loop to print metadata for each document in the data list\n",
    "# for document in data:\n",
    "#     print(f\"Metadata for document in {document.metadata['source']}:\\n{document.metadata}\\n\")\n",
    "\n",
    "# # example loop to print the length of each text chunk\n",
    "# for i, chunk in enumerate(text_chunks):\n",
    "#     print(f\"Length of text chunk {i}: {len(chunk.page_content)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# We need to get the OpenAI or Azure OpenAI API key. This is how we use & get charged for LLM usage\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "else:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone is a service that will take the documents that you split\n",
    "# And store the embedding vectors (a math construct that tells LLMs where in the model to find similar words)\n",
    "# Currently, Pinecone is free for use cases like this. Other vector stores exist; FAISS, ChromaDB, etc.\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "PINECONE_API_ENV = \"us-central1-gcp\"\n",
    "\n",
    "# We need to get the Pinecone API key.\n",
    "if \"PINECONE_API_KEY\" in os.environ:\n",
    "    PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "else:\n",
    "    PINECONE_API_KEY = getpass(\"Enter your Pinecone API Key: \")\n",
    "    os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "index_name = 'aaa-reports-002'\n",
    "\n",
    "# Remove our Pinecone Index if it exists and create a new one.\n",
    "if process_index_rebuild:\n",
    "    try:\n",
    "        pinecone.delete_index(name=index_name)\n",
    "    except:\n",
    "        print(f\"The index {index_name} does not exist.\")\n",
    "    # Create an index (a database) for our embeddings\n",
    "    pinecone.create_index(name=index_name, dimension=1536, metric=\"cosine\")\n",
    "    print(f\"The index {index_name} was created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "# Let's create the embeddings (vector math pointers of our docs) using OpenAI's Embeddings Creator model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "# create an empty list to store the metadata for each document\n",
    "metadata_list = []\n",
    "\n",
    "# loop through the text_chunks and set the metadata for each document\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    # get the filename of the source text file from the metadata of the first page in the chunk\n",
    "    filename = chunk.metadata['source'].split(os.sep)[-1].split('.')[0]\n",
    "    # set the sources metadata key to the filename\n",
    "    metadata = {\"source\": filename}\n",
    "    # add the metadata dictionary to the metadata_list\n",
    "    metadata_list.append(metadata)\n",
    "\n",
    "# Create the docsearch which both builds the index and provides us an object to use the index.\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, metadatas=metadata_list, index_name=index_name, namespace=namespace_chunk_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: QA + Stuffed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_text': '\\nThe issue with the SEABEEs uniform color was that they were the only coalition force in theater wearing the utility green uniform, which caused them to stand out and be stopped at check points and harassed. It also caused incidents of mistaken identity with potentially serious consequences, and caused uniform resupply problems in non-NCF fleet hospital and public works augment units.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "query = \"What was the issue with the SEABEEs uniform color?\"\n",
    "\n",
    "qa_stuff_prompt_template = \"\"\"You are a military specialist. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer as a military expert in after-action reports.: \"\"\"\n",
    "QA_STUFF_PROMPT = PromptTemplate(\n",
    "    template=qa_stuff_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "temperature = 0.2\n",
    "top_k_results = 5\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"davinci\",\n",
    "    temperature=temperature,\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    max_tokens=-1\n",
    ")\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "docs_to_search = docsearch.similarity_search(\n",
    "    query=query, k=top_k_results, filter=metadata_filter, namespace=\"chunk_size_1024\"\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=QA_STUFF_PROMPT)\n",
    "chain({\"input_documents\": docs_to_search, \"question\": query}, return_only_outputs=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Summarize + Map_Reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "query = \"What was the issue with the SEABEEs uniform color?\"\n",
    "\n",
    "summarize_mr_prompt_template = \"\"\"Write a summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "As a military specialist in after-action reports. \n",
    "\"\"\"\n",
    "SUMMARIZE_MR_PROMPT = PromptTemplate(template=summarize_mr_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "temperature = 0.2\n",
    "top_k_results = 5\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"davinci\",\n",
    "    temperature=temperature,\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    max_tokens=-1\n",
    ")\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "docs_to_search = docsearch.similarity_search(\n",
    "    query=query, k=top_k_results, filter=metadata_filter, namespace=\"chunk_size_1024\"\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(OpenAI(temperature=0), chain_type=\"map_reduce\", return_intermediate_steps=True, map_prompt=SUMMARIZE_MR_PROMPT, combine_prompt=SUMMARIZE_MR_PROMPT)\n",
    "chain({\"input_documents\": docs_to_search}, return_only_outputs=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: QA + Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "refine_prompt_template = (\n",
    "    \"The original question is as follows: {question}\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question. \"\n",
    "    \"If the context isn't useful, return the original answer. Reply as a military specialist in after-action report. \"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"existing_answer\", \"context_str\"],\n",
    "    template=refine_prompt_template,\n",
    ")\n",
    "\n",
    "\n",
    "initial_qa_template = (\n",
    "    \"Context information is below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {question}\"\n",
    ")\n",
    "initial_qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context_str\", \"question\"], template=initial_qa_template\n",
    ")\n",
    "\n",
    "query = \"How long was the air war?\"\n",
    "\n",
    "temperature = 0.2\n",
    "top_k_results = 5\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "llm = OpenAI(\n",
    "    model_name=\"davinci\",\n",
    "    temperature=temperature,\n",
    "    openai_api_key=OPENAI_API_KEY, \n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "docsearch = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "\n",
    "docs_to_search = docsearch.similarity_search(\n",
    "    query=query, k=top_k_results, filter=metadata_filter, namespace=\"chunk_size_512\"\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"refine\", return_refine_steps=True, question_prompt=initial_qa_prompt, refine_prompt=refine_prompt)\n",
    "chain({\"input_documents\": docs_to_search, \"question\": query}, return_only_outputs=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of map_reduce with a prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Define custom prompt templates for the question-answering chain\n",
    "question_prompt_template = (\n",
    "    \"You are an AI language model trained to analyze military after-action reports. \"\n",
    "    \"Your expertise includes military doctrine, identifying issues and recommendations, \"\n",
    "    \"understanding changes in future strategies, analyzing tactics, and evaluating logistics. \"\n",
    "    \"Given the following excerpt from a document, determine if it contains relevant information to answer the question.\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "QUESTION_PROMPT = PromptTemplate(template=question_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "combine_prompt_template = (\n",
    "    \"As an AI language model with a deep understanding of military after-action reports, \"\n",
    "    \"synthesize the following extracted parts of a document to create a comprehensive and informative answer to the question. \"\n",
    "    \"If you don't know the answer, just say that you don't know. Don't try to make up an answer.\\n\\n\"\n",
    "    \"QUESTION: {question}\\n\"\n",
    "    \"=========\\n\"\n",
    "    \"{summaries}\\n\"\n",
    "    \"=========\"\n",
    ")\n",
    "COMBINE_PROMPT = PromptTemplate(template=combine_prompt_template, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "# Set up the metadata filter\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "# Set up search parameters\n",
    "test_k = 20\n",
    "test_query_question = \"What were the top 5 problems, issues, or lessons?\"\n",
    "\n",
    "# Search for similar documents\n",
    "test_docs_to_search_with_score = docsearch.similarity_search_with_score(query=test_query_question, k=test_k, filter=metadata_filter, namespace=\"chunk_size_512\")\n",
    "\n",
    "# Set up the language model\n",
    "test_temp = 0.2\n",
    "llm = OpenAI(temperature=test_temp, openai_api_key=OPENAI_API_KEY, max_tokens=2048)\n",
    "\n",
    "# Load the question-answering chain\n",
    "chain = load_qa_chain(llm=llm, chain_type=\"map_reduce\", return_map_steps=True, question_prompt=QUESTION_PROMPT, combine_prompt=COMBINE_PROMPT)\n",
    "\n",
    "# Extract only the Document objects from the list of tuples\n",
    "test_docs_to_search = [doc_score_tuple[0] for doc_score_tuple in test_docs_to_search_with_score]\n",
    "\n",
    "# Run the chain with the input documents and question\n",
    "res = chain({\"input_documents\": test_docs_to_search, \"question\": test_query_question}, return_only_outputs=False)\n",
    "\n",
    "# Display the result\n",
    "res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Custom Prompt and the Summarization Model and Refine Chain Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Write a summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "\n",
    "# Set up the language model\n",
    "test_temp = 0.2\n",
    "llm = OpenAI(temperature=test_temp, openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Set up the metadata filter\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "# Set up search parameters\n",
    "test_k = 5\n",
    "test_query_question = \"SEABEE uniform issues.\"\n",
    "\n",
    "# Search for similar documents\n",
    "test_docs_to_search_with_score = docsearch.similarity_search(query=test_query_question, k=test_k, filter=metadata_filter, namespace=\"chunk_size_512\")\n",
    "\n",
    "# Extract only the Document objects from the list of tuples\n",
    "#test_docs_to_search = [doc_score_tuple[0] for doc_score_tuple in test_docs_to_search_with_score]\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"refine\", return_intermediate_steps=False, question_prompt=PROMPT, refine_prompt=refine_prompt)\n",
    "chain({\"input_documents\": test_docs_to_search}, return_only_outputs=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the SUMMARIZE - Refine Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "temperature = 0.2\n",
    "llm = OpenAI(temperature=0.3, openai_api_key=OPENAI_API_KEY, max_tokens=512)\n",
    "\n",
    "summarization_question = \"Summarize the issues related to the SEABEE uniform color.\"\n",
    "\n",
    "metadata_filter = {\"source\": \"aar-desertStorm_text_cleaned\"}\n",
    "\n",
    "top_k_results = 5\n",
    "\n",
    "docs_to_search = docsearch.similarity_search(\n",
    "    query=summarization_question,\n",
    "    k=top_k_results,\n",
    "    include_metadata=True,\n",
    "    namespace=\"chunk_size_512\",\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(llm=llm, chain_type=\"refine\")\n",
    "\n",
    "chain(docs_to_search)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use our connection to the LLM and ask it questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python31\\Lib\\site-packages\\gradio\\deprecation.py:43: UserWarning: You have unused kwarg parameters in Radio, please remove them: {'default': 'Summarization'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the pre-trained question-answering LLM model using the langchain library\n",
    "llm = OpenAI(temperature=0.3, openai_api_key=OPENAI_API_KEY, max_tokens=512)\n",
    "\n",
    "# Create a list of unique document titles from the metadata of the text chunks\n",
    "doc_titles = list(set(metadata['source'] for metadata in metadata_list))\n",
    "\n",
    "# Define custom prompt template: QA-STUFF Prompt\n",
    "qa_stuff_prompt_template = \"\"\"You are a military specialist. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer as a military expert in after-action reports.: \"\"\"\n",
    "QA_STUFF_PROMPT = PromptTemplate(\n",
    "    template=qa_stuff_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define custom prompt template: SUMMARIZE-MAP_REDUCE Prompt\n",
    "summarize_mr_prompt_template = \"\"\"Write a summary of the following:\n",
    "\n",
    "\n",
    "{text}\n",
    "\n",
    "As a military specialist in after-action reports. \n",
    "\"\"\"\n",
    "SUMMARIZE_MR_PROMPT = PromptTemplate(template=summarize_mr_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "# Create our CHAIN object for QA_STUFFed Prompts\n",
    "chain_qa = load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=QA_STUFF_PROMPT)\n",
    "\n",
    "# Create our CHAIN object for SUMMARIZED_MR Prompts\n",
    "chain_summarize = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", return_intermediate_steps=False, map_prompt=SUMMARIZE_MR_PROMPT, combine_prompt=SUMMARIZE_MR_PROMPT)\n",
    "\n",
    "# Create our Function to process QA (mode) prompts\n",
    "def qa_function(query, doc_title, temperature, k):\n",
    "\n",
    "    # Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "    \n",
    "    metadata_filter = {\"source\": doc_title}\n",
    "    \n",
    "    qa_docs_to_search_with_scores = docsearch.similarity_search_with_score(\n",
    "        query=query, k=k, filter=metadata_filter, namespace=\"chunk_size_512\"\n",
    "    )\n",
    "\n",
    "    # Separate the documents from their scores\n",
    "    qa_docs_to_search = [doc for doc, score in qa_docs_to_search_with_scores]\n",
    "\n",
    "    # Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "    llm.temperature = temperature\n",
    "    output = chain_qa({\"input_documents\": qa_docs_to_search, \"question\": query}, return_only_outputs=True)\n",
    "\n",
    "    # Extract the actual answer text from the output dictionary\n",
    "    answer = output['output_text']\n",
    "\n",
    "    # Extract the docs that the similarity_search found, scored\n",
    "    truncated_docs_str = \"\"\n",
    "    for index, (doc, score) in enumerate(qa_docs_to_search_with_scores):\n",
    "        truncated_text = doc.page_content[:200].replace(\"\\n\", \"\")\n",
    "        truncated_docs_str += f\"Doc {index + 1} snippet (score: {score:.2f}): {truncated_text}\\n------\\n\"\n",
    "\n",
    "    return answer, truncated_docs_str\n",
    "\n",
    "\n",
    "# Create our Function to process Summarization (mode) prompts\n",
    "def summarization_function(query, doc_title, temperature, k):\n",
    "\n",
    "    # Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "    \n",
    "    metadata_filter = {\"source\": doc_title}\n",
    "    \n",
    "    sum_docs_to_search_with_scores = docsearch.similarity_search_with_score(\n",
    "        query=query, k=k, filter=metadata_filter, namespace=\"chunk_size_512\"\n",
    "    )\n",
    "\n",
    "    # Separate the documents from their scores\n",
    "    sum_docs_to_search = [doc for doc, score in sum_docs_to_search_with_scores]\n",
    "\n",
    "    # Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "    llm.temperature = temperature\n",
    "    output = chain_summarize({\"input_documents\": sum_docs_to_search}, return_only_outputs=False)\n",
    "\n",
    "    # Extract the actual answer text from the output dictionary\n",
    "    answer = output['output_text']\n",
    "\n",
    "    # Extract the docs that the similarity_search found, scored\n",
    "    truncated_docs_str = \"\"\n",
    "    for index, (doc, score) in enumerate(sum_docs_to_search_with_scores):\n",
    "        truncated_text = doc.page_content[:200].replace(\"\\n\", \"\")\n",
    "        truncated_docs_str += f\"Doc {index + 1} snippet (score: {score:.2f}): {truncated_text}\\n------\\n\"\n",
    "\n",
    "    return answer, truncated_docs_str\n",
    "\n",
    "\n",
    "# Function to determine which Function to use: qa or summarization along with the vars needed (order must match the list order provided to gradio)\n",
    "def process_input(mode, query, doc_title, temperature, k):\n",
    "    if mode == \"Question/Answer\":\n",
    "        return qa_function(query, doc_title, temperature, k)\n",
    "    elif mode == \"Summarization\":\n",
    "        return summarization_function(query, doc_title, temperature, k)\n",
    "    else:\n",
    "        return \"Invalid mode selected\", \"\"\n",
    "\n",
    "\n",
    "# The main function to launch the gradio interface\n",
    "def main():\n",
    "    with gr.Interface(\n",
    "        fn=process_input,\n",
    "        inputs=[\n",
    "            gr.Radio(choices=[\"Question/Answer\", \"Summarization\"], label=\"Model Interaction Mode\", default=\"Summarization\"),\n",
    "            gr.Textbox(lines=1, label=\"Question\"),\n",
    "            gr.Dropdown(choices=doc_titles, label=\"Filter by AAR title\", info=\"Filter by after-action report name\"),\n",
    "            gr.Slider(0, 1, step=0.1, value=0, label=\"Model Temp.\", info=\"0=More Precise, 1=Greater degree of freedom\"),\n",
    "            gr.Slider(3, 20, step=1, value=5, label=\"Docs to Search\", info=\"Number of AAR document 'chunks' to search and summarize\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.Textbox(label=\"Answer\", lines=10),\n",
    "            gr.Textbox(label=\"Docs presented to the LLM\", info=\"A list of document 'chunks' from the AAR with its question-similarity score.\"),\n",
    "        ],\n",
    "        title=\"After-Action Report ChatGPT\",\n",
    "        examples=[\n",
    "            [\"Summarization\", \"Echeloning and it's use in Desert Shield and Storm.\", \"aar-desertStorm_text_cleaned\"],\n",
    "            [\"Summarization\", \"Issues related to SEABEE uniforms.\", \"aar-desertStorm_text_cleaned\"],\n",
    "            [\"Question/Answer\", \"How long did the air war last?\", \"aar-desertStorm_text_cleaned\"]\n",
    "        ],\n",
    "    ) as iface:\n",
    "        iface.launch()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREDIT: Many thanks to Gkamradt's work here: https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/Ask%20A%20Book%20Questions.ipynb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the issue with the SEABEEs uniform color?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
