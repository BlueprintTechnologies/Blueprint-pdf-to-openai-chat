{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<src img='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai_1.png'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pdf2image pytesseract reportlab pinecone-client"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our data (the process of OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use one of the Content Loaders from Langchain to read our OCR'd text file\n",
    "loader = TextLoader(\"./text_output/cleaned_file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some details about our input text document\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original doc has **far** too many chartacters to send to our LLM\n",
    "# So, we break down the doc into multiple documents. \n",
    "# Experiment with the chunk_size accordingly. \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print (f'Now you have {len(texts)} documents that will be sent to the \\n'\n",
    "       f'LLM when needed to fulfill the answer to a question'\n",
    "       )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# We need to get the OpenAI or Azure OpenAI API key. This is how we use & get charged for LLM usage\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "else:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone is a service that will take the documents that you split\n",
    "# And store the embedding vectors (a math construct that tells LLMs where in the model to find similar words)\n",
    "# Currently, Pinecone is free for use cases like this. Other vector stores exist; FAISS, ChromaDB, etc.\n",
    "\n",
    "PINECONE_API_ENV = \"us-east4-gcp\"\n",
    "\n",
    "try:\n",
    "    PINECONE_API_KEY\n",
    "except NameError:\n",
    "    PINECONE_API_KEY = getpass(\"Enter your Pinecone API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# Let's create the embeddings (vector math pointers of our docs) using OpenAI's Embeddings Creator model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "index_name = \"ds-after-action01\"\n",
    "\n",
    "# Let's send our embeddings to Pinecone for temp storage and usage\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the issue with the SEABEA uniform color?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "num_docs = len(docs)\n",
    "print(f'There are {num_docs} documents out of the {len(texts)} produced, or split \\n'\n",
    "      f'from the original doc that are relevant (similar) to your search term')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's set up our connection to the LLM so we can ask it questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Set up the connection to the OpenAI LLM, with parameters that control its behavior\n",
    "llm = OpenAI(\n",
    "    temperature=0, # control the degree of creative or realism for the model (0-1)\n",
    "    openai_api_key=OPENAI_API_KEY, # our key to OpenAI or Azure's OpenAI LLM\n",
    "    max_tokens=-1 # the number of tokens to return, -1 == max\n",
    "    )\n",
    "\n",
    "# Setup the 'chain' of documents that are \"stuffed\" (literally) into the LLM as question-time.\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\") # there are other types of chain_type. Experiment with map_reduce."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use our connection to the LLM and ask it questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our question or query\n",
    "query = \"Brief me on the issues related to SEABEE uniform color? Cite the doc sources.\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our question or query\n",
    "query = \"Extract the key individual's names and ranks from the doc. Include LTG, VADM, CAPT, FOCM ranks\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our question or query\n",
    "query = \"How many Seabee reservists were serving?\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our question or query\n",
    "query = \"Regarding NAVAL CONSTRUCTION FORCE (NCF) CAPABILITY category, summarize each problem/issues/lesson. Cite your source\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
