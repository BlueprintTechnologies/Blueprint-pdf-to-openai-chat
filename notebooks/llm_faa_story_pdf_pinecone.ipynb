{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<src img='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/Evogelpohl/linkArtifacts/main/pdf_openai_2.png'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q pdf2image pytesseract reportlab pinecone-client Pillow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image, ImageOps\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets define some key variable for the PDF OCR'ing process\n",
    "#Configure paths and variables\n",
    "\n",
    "TESSERACT_PATH = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\" #tesseract must be installed separately\n",
    "POPPLER_PATH = r\"C:\\Program Files\\poppler-23.01.0\\Library\\bin\" #poppler must be installed separately\n",
    "\n",
    "LLM_DIRECTORY = Path(r\"C:\\Temp\\pdf_to_openai_chat\")\n",
    "LLM_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SRC_PDFS_DIRECTORY = LLM_DIRECTORY / \"src_pdfs\"\n",
    "\n",
    "PAGES_DIRECTORY = LLM_DIRECTORY / \"pages\"\n",
    "PAGES_DIRECTORY.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_OUTPUT_DIRECTORY = LLM_DIRECTORY / \"text_output\"\n",
    "TEXT_OUTPUT_DIRECTORY.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF image OCR process, saving the results to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == \"Windows\":\n",
    "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH\n",
    "\n",
    "def create_page_images_pdf2image(pdf_path, output_directory, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a PDF file into a list of JPEG images, one for each page.\n",
    "    \n",
    "    :param pdf_path: Path to the input PDF file.\n",
    "    :param output_directory: Directory to save the generated JPEG images.\n",
    "    :param dpi: DPI for the generated images.\n",
    "    :return: A list of file paths for the generated images.\n",
    "    \"\"\"\n",
    "    image_file_list = []\n",
    "    convert_args = {\"pdf_path\": pdf_path, \"dpi\": dpi}\n",
    "\n",
    "    if platform.system() == \"Windows\":\n",
    "        convert_args[\"poppler_path\"] = POPPLER_PATH\n",
    "\n",
    "    pdf_pages = convert_from_path(**convert_args)\n",
    "\n",
    "    for page_number, page in enumerate(pdf_pages, start=1):\n",
    "        output_file = output_directory / f\"page_{page_number:03}.jpg\"\n",
    "        page.save(output_file, \"JPEG\")\n",
    "        image_file_list.append(output_file)\n",
    "        print(f\"Image created: {output_file}\")\n",
    "\n",
    "    return image_file_list\n",
    "\n",
    "\n",
    "def convert_to_bw(image_list):\n",
    "    \"\"\"\n",
    "    Convert a list of images to black and white.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    \"\"\"\n",
    "    for image_file in image_list:\n",
    "        image = Image.open(image_file)\n",
    "        gray_image = ImageOps.grayscale(image)\n",
    "        bw_image = gray_image.point(lambda x: 0 if x < 128 else 255, '1')\n",
    "        bw_image.save(image_file)\n",
    "\n",
    "\n",
    "def ocr_images(image_list, text_output_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on a list of images and append the extracted text to a file.\n",
    "    \n",
    "    :param image_list: List of input image file paths.\n",
    "    :param text_output_path: File path to save the extracted text.\n",
    "    \"\"\"\n",
    "    with open(text_output_path, \"a\") as output_file:\n",
    "        for image_file in image_list:\n",
    "            text = str(((pytesseract.image_to_string(Image.open(image_file)))))\n",
    "            text = text.replace(\"-\\n\", \"\")\n",
    "            output_file.write(text)\n",
    "            print(f\"OCR processed: {image_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_files = SRC_PDFS_DIRECTORY.glob(\"*.pdf\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_name = pdf_file.stem\n",
    "        pages_subdir = PAGES_DIRECTORY / pdf_name\n",
    "        pages_subdir.mkdir(parents=True, exist_ok=True)\n",
    "        image_file_list = create_page_images_pdf2image(pdf_file, pages_subdir, dpi=300)\n",
    "        convert_to_bw(image_file_list)\n",
    "        text_output_file = TEXT_OUTPUT_DIRECTORY / f\"{pdf_name}_text.txt\"\n",
    "  \n",
    "        ocr_images(image_file_list, text_output_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our data (the process of OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use one of the Content Loaders from Langchain to read our OCR'd text file\n",
    "loader = TextLoader(r\"C:\\Temp\\pdf_to_openai_chat\\text_output\\Troubled_Passage_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 1467780 characters in your document\n"
     ]
    }
   ],
   "source": [
    "# Let's see some details about our input text document\n",
    "data = loader.load()\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on our chunk_size, we now have 650 documents that will be sent to the \n",
      "LLM when needed to fulfill the answer to a question\n"
     ]
    }
   ],
   "source": [
    "# The original doc has **far** too many chartacters to send to our LLM\n",
    "# So, we break down the doc into multiple documents. \n",
    "# Experiment with the chunk_size accordingly. \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(data)\n",
    "print (f'Based on our chunk_size, we now have {len(texts)} documents that will be sent to the \\n'\n",
    "       f'LLM when needed to fulfill the answer to a question'\n",
    "       )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the embeddings of our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# We need to get the OpenAI or Azure OpenAI API key. This is how we use & get charged for LLM usage\n",
    "if \"OPENAI_API_KEY\" in os.environ:\n",
    "    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "else:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone is a service that will take the documents that you split\n",
    "# And store the embedding vectors (a math construct that tells LLMs where in the model to find similar words)\n",
    "# Currently, Pinecone is free for use cases like this. Other vector stores exist; FAISS, ChromaDB, etc.\n",
    "\n",
    "PINECONE_API_ENV = \"us-east4-gcp\"\n",
    "\n",
    "try:\n",
    "    PINECONE_API_KEY\n",
    "except NameError:\n",
    "    PINECONE_API_KEY = getpass(\"Enter your Pinecone API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "\n",
    "# Let's create the embeddings (vector math pointers of our docs) using OpenAI's Embeddings Creator model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "\n",
    "# Create an index (a database) for our embeddings\n",
    "index_name = 'faa-story-002'\n",
    "pinecone.create_index(name=\"faa-story-002\", dimension=1536, metric=\"cosine\")\n",
    "\n",
    "# Let's send our embeddings to Pinecone for temp storage and usage\n",
    "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 documents out of the 650 produced, or splitfrom the original doc that are relevant (similar) to your search term\n"
     ]
    }
   ],
   "source": [
    "# We'll create a sample query and use the similarity_search to determine what docs are relavent to our query\n",
    "query = \"Summarize the Author’s Preface\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "num_docs = len(docs)\n",
    "print(f'There are {num_docs} documents out of the {len(texts)} produced, or split'\n",
    "      f'from the original doc that are relevant (similar) to your search term')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's set up our connection to the LLM so we can ask it questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Set up the connection to the OpenAI LLM, with parameters that control its behavior\n",
    "llm = OpenAI(\n",
    "    temperature=0, # control the degree of creative or realism for the model (0-1)\n",
    "    openai_api_key=OPENAI_API_KEY, # our key to OpenAI or Azure's OpenAI LLM\n",
    "    max_tokens=-1 # the number of tokens to return, -1 == max\n",
    "    )\n",
    "\n",
    "# Setup the 'chain' of documents that are \"stuffed\" (literally) into the LLM as question-time.\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\") # there are other types of chain_type. Experiment with map_reduce."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use our connection to the LLM and ask it questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The author expresses his appreciation for the assistance he received from many individuals in researching and writing the book, including Nick A. Komons, Glyndon P. Bennett, Gerald E. Lavey, John G. Leyden, Billy E. Bays, Claude S. Brinegar, William T. Coleman, Jr., John H. Shaffer, Alexander P. Butterfield, John L. McLucas, James E. Dow, Charles O. Cary, John F. Leyden, Richard P. Hallion, William M. Leary, and Richard K. Smith. He also expresses his appreciation for the help of librarians and archivists at various institutions.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our question or query\n",
    "query = \"Summarize the author's preface.\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Nick A. Komons, Glyndon P. Bennett, Gerald E. Lavey, John G. Leyden, Billy E. Bays, Claude S. Brinegar, William T. Coleman, Jr., John H. Shaffer, Alexander P. Butterfield, John L. McLucas, James E. Dow, and Oscar Bakke.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our question or query\n",
    "query = \"Extract key individual's names from this text\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Flight 514 was a Boeing 727 en route to the Capital from the Midwest. After consulting his dispatcher, the captain decided to change his destination to Dulles International. The aircraft struck the crest of a ridge near the town of Berryville, killing all aboard.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our question or query\n",
    "query = \"Tell me about Flight 514 to Midwest including how many passengers were killed.\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In the autumn of 1972, two violent hijacking incidents occurred. In October, four men wanted for murder and bank robbery killed a ticket agent and made their way to Havana aboard an Eastern 727. In November, three fugitives seized a Southern Airways DC-9 and embarked on a harrowing journey, making eight landings, collecting ransom, and threatening to crash the aircraft into a nuclear facility. The FBI eventually shot the tires of the DC-9, and the pilot managed to take off and land safely in Havana. These incidents broke the diplomatic impasse and led to a bilateral agreement between the United States and Cuba in February 1973, which included a commitment to return or prosecute hijackers.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define our question or query\n",
    "query = \"Summarize the hijacking incidents\"\n",
    "\n",
    "# Look in the embeddings store for documents (splits of the orig text) that are similar to your question\n",
    "docs_to_search = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "# Send the matching docs & our question to the LLM. It will return the answer below.\n",
    "chain.run(input_documents=docs_to_search, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove our Pinecone Index\n",
    "\n",
    "pinecone.delete_index(name=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
